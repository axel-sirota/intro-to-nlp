{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1bVXQRQJIETKyMSok4cMTu1gmh-ud4tJP","timestamp":1693947689629}],"authorship_tag":"ABX9TyOvb0+iOswCb0rpLCNoiMKq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Text Classification"],"metadata":{"id":"6cTEz_RPB9UW"}},{"cell_type":"markdown","source":["## Installs and stuff"],"metadata":{"id":"vHeVVODNCBjV"}},{"cell_type":"code","source":["!pip install --upgrade textblob spacy 'gensim==4.2.0' swifter"],"metadata":{"id":"4Ky4U5WZCDeP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!python -m textblob.download_corpora lite\n","!python -m spacy download en_core_web_sm"],"metadata":{"id":"7P8pF-WECEQP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import scipy as sp\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.naive_bayes import MultinomialNB         # Naive Bayes\n","from sklearn.linear_model import LogisticRegression\n","from sklearn import metrics\n","from textblob import TextBlob, Word\n","from nltk.stem.snowball import SnowballStemmer\n","\n","import spacy\n","import gensim\n","import warnings\n","import nltk\n","import re\n","warnings.filterwarnings('ignore')\n","nltk.download('punkt')\n","textblob_tokenizer = lambda x: TextBlob(x).words\n"],"metadata":{"id":"N7jR_ed_CES0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%writefile get_data.sh\n","if [ ! -f yelp.csv ]; then\n","  wget -O yelp.csv https://www.dropbox.com/s/xds4lua69b7okw8/yelp.csv?dl=0\n","fi"],"metadata":{"id":"k6RoBSKKLvJw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!bash get_data.sh"],"metadata":{"id":"OcmMkgZqLxqW"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-zG2McSWHUCK"},"outputs":[],"source":["# Read yelp.csv into a DataFrame.\n","path = './yelp.csv'\n","yelp = pd.read_csv(path)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KVOjA6xxHUCK"},"outputs":[],"source":["# The head of the original data\n","yelp.head()"]},{"cell_type":"code","source":["\n","# Create a new DataFrame that only contains the 5-star and 1-star reviews.\n","yelp_best_worst = yelp[(yelp.stars==5) | (yelp.stars==1)]\n","\n","# Define X and y.\n","X = yelp_best_worst.text\n","y = yelp_best_worst.stars\n","\n","# Split the new DataFrame into training and testing sets.\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25 ,random_state=99)"],"metadata":{"id":"GIy59uyhCQFV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Naive Bayes Classifier"],"metadata":{"id":"rVbPd0pxCor5"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"9nHHONYbHUCO"},"outputs":[],"source":["# Use default options for CountVectorizer.\n","vect = CountVectorizer()\n","\n","# Create document-term matrices.\n","X_train_dtm = vect.fit_transform(X_train)\n","X_test_dtm = vect.transform(X_test)\n","\n","# Use Naive Bayes to predict the star rating.\n","nb = MultinomialNB()\n","nb.fit(X_train_dtm, y_train)\n","y_pred_class = nb.predict(X_test_dtm)\n","\n","# Calculate accuracy.\n","print((metrics.accuracy_score(y_test, y_pred_class)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WnAT7w6WHUCO"},"outputs":[],"source":["y_test.value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rVzLrEQEHUCP"},"outputs":[],"source":["# Calculate null accuracy.\n","y_test_binary = np.where(y_test==5, 1, 0) # five stars become 1, one stars become 0\n","print('Percent 5 Stars:', y_test_binary.mean())\n","print('Percent 1 Stars:', 1 - y_test_binary.mean())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q_Qnptd4HUCP"},"outputs":[],"source":["from sklearn.metrics import classification_report\n","print(classification_report(y_test, y_pred_class))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wpMGU7NZHUCP"},"outputs":[],"source":["print(classification_report(y_test, [5 for _ in y_test]))"]},{"cell_type":"markdown","metadata":{"id":"_AtpXGiQHUCP"},"source":["Our model predicted ~92% accuracy, which is an improvement over this baseline 82% accuracy (assuming our model always predicts 5 stars).\n","\n","Let's look more into how the vectorizer works."]},{"cell_type":"code","source":["X_train.shape"],"metadata":{"id":"o4EsCuFcd7s9"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LCPzaionHUCQ"},"outputs":[],"source":["# Notice how the data was transformed into this sparse matrix with 1,022 datapoints and 16,825 features!\n","#   - Recall that vectorizations of text will be mostly zeros, since only a few unique words are in each document.\n","#   - For that reason, instead of storing all the zeros we only store non-zero values (inside the 'sparse matrix' data structure!).\n","#   - We have 3064 Yelp reviews in our training set.\n","#   - 16,825 unique words were found across all documents.\n","\n","X_train_dtm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ly3AalumHUCQ"},"outputs":[],"source":["# Let's take a look at the vocabulary that was generated, containing 16,825 unique words.\n","#   'vocabulary_' is a dictionary that converts each word to its index in the sparse matrix.\n","#   - For example, the word \"four\" is index #3230 in the sparse matrix.\n","\n","len(vect.vocabulary_)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QSJiS4YGHUCQ"},"outputs":[],"source":["# Finally, let's convert the sparse matrix to a typical ndarray using .toarray()\n","#   - Remember, this takes up a lot more memory than the sparse matrix! However, this conversion is sometimes necessary.\n","\n","X_train_dtm.toarray().shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VgUbZFqjHUCR"},"outputs":[],"source":["X, y"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ss5gLwaCHUCR"},"outputs":[],"source":["# We will use this function below for simplicity.\n","\n","# Define a function that accepts a vectorizer and calculates the accuracy.\n","def tokenize_test(vect):\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n","\n","    X_train_dtm = vect.fit_transform(X_train)\n","    X_test_dtm = vect.transform(X_test)\n","\n","    print((f'Features: {X_train_dtm.shape[1]}'))\n","\n","    nb = MultinomialNB()\n","    nb.fit(X_train_dtm, y_train)\n","\n","    y_pred_class = nb.predict(X_test_dtm)\n","\n","    print(f'Accuracy: {metrics.accuracy_score(y_test, y_pred_class)}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UEXuowl-HUCR"},"outputs":[],"source":["vect"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hp1q1FplHUCR"},"outputs":[],"source":["# min_df ignores words that occur less than twice ('df' means \"document frequency\").\n","vect = CountVectorizer(min_df=2, max_features=10000)\n","tokenize_test(vect)"]},{"cell_type":"markdown","metadata":{"id":"fHaTzZDHHUCS"},"source":["Let's take a look next at other ways of preprocessing text!\n","\n","- **Objective:** Demonstrate common text preprocessing techniques.\n","\n","<a id='ngrams'></a>\n","### N-Grams"]},{"cell_type":"markdown","metadata":{"id":"VINoRJSnHUCS"},"source":["N-grams are features which consist of N consecutive words. This is useful because using the bag-of-words model, treating `data scientist` as a single feature has more meaning than having two independent features `data` and `scientist`!\n","\n","Example:\n","```\n","my cat is awesome\n","Unigrams (1-grams): 'my', 'cat', 'is', 'awesome'\n","Bigrams (2-grams): 'my cat', 'cat is', 'is awesome'\n","Trigrams (3-grams): 'my cat is', 'cat is awesome'\n","4-grams: 'my cat is awesome'\n","```\n","\n","- **ngram_range:** tuple (min_n, max_n)\n","- The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n <= n <= max_n will be used."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rz06AwDOHUCS"},"outputs":[],"source":["# Include 1-grams and 2-grams.\n","vect = CountVectorizer(ngram_range=(1,2))\n","X_train_dtm = vect.fit_transform(X_train)\n","X_train_dtm.shape"]},{"cell_type":"markdown","metadata":{"id":"sKtUxR3JHUCT"},"source":["We can start to see how supplementing our features with n-grams can lead to more feature columns. When we produce n-grams from a document with $W$ words, we add an additional $(n-W+1)$ features (at most). That said, be careful â€” when we compute n-grams from an entire corpus, the number of _unique_ n-grams could be vastly higher than the number of _unique_ unigrams! This could cause an undesired feature explosion.\n","\n","Although we sometimes add important new features that have meaning such as `data scientist`, many of the new features will just be noise. So, particularly if we do not have much data, adding n-grams can actually decrease model performance. This is because if each n-gram is only present once or twice in the training set, we are effectively adding mostly noisy features to the mix."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"ElKoBWZZHUCT"},"outputs":[],"source":["# Last 50 features\n","print((vect.get_feature_names_out()[-25:]))"]},{"cell_type":"markdown","metadata":{"id":"e8Y2mh_5HUCT"},"source":["<a id='stopwords'></a>\n","\n","### Stop-Word Removal\n","\n","- **What:** This process is used to remove common words that will likely appear in any text.\n","- **Why:** Because common words exist in most documents, they likely only add noise to your model and should be removed.\n","\n","**What are stop words?**\n","Stop words are some of the most common words in a language. They are used so that a sentence makes sense grammatically, such as prepositions and determiners, e.g., \"to,\" \"the,\" \"and.\" However, they are so commonly used that they are generally worthless for predicting the class of a document. Since \"a\" appears in spam and non-spam emails, for example, it would only contribute noise to our model.\n","\n","Example:\n","\n","> 1. Original sentence: \"The dog jumped over the fence\"  \n","> 2. After stop-word removal: \"dog jumped over fence\"\n","\n","The fact that there is a fence and a dog jumped over it can be derived with or without stop words."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nL0aCjO9HUCU"},"outputs":[],"source":["# Show vectorizer options.\n","vect"]},{"cell_type":"markdown","metadata":{"id":"0k5g_83WHUCU"},"source":["- **stop_words:** string {`english`}, list, or None (default)\n","- If `english`, a built-in stop word list for English is used.\n","- If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens.\n","- If None, no stop words will be used. `max_df` can be set to a value in the range [0.7, 1.0) to automatically detect and filter stop words based on intra corpus document frequency of terms. (If `max_df` = 0.7, then if > 70% of documents contain a word it will not be included in the feature set!)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FI3Ebh2bHUCU"},"outputs":[],"source":["help(vect)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VboHUvv5HUCU"},"outputs":[],"source":["# Remove English stop words.\n","vect = CountVectorizer(stop_words='english')\n","tokenize_test(vect)\n","vect.get_params()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lsGW0EskHUCU"},"outputs":[],"source":["# Set of stop words\n","print((vect.get_stop_words()))"]},{"cell_type":"markdown","metadata":{"id":"lkU9h63mHUCV"},"source":["<a id='cvec_opt'></a>\n","### Other CountVectorizer Options"]},{"cell_type":"markdown","metadata":{"id":"-yYh6vfZHUCV"},"source":["- `max_features`: int or None, default=None\n","- If not None, build a vocabulary that only consider the top `max_features` ordered by term frequency across the corpus. This allows us to keep more common n-grams and remove ones that may appear once. If we include words that only occur once, this can lead to said features being highly associated with a class and cause overfitting."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZzcB7ICkHUCV"},"outputs":[],"source":["# Remove English stop words and only keep 100 features.\n","vect = CountVectorizer(stop_words='english', max_features=100)\n","tokenize_test(vect)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tyeDZcfhHUCV"},"outputs":[],"source":["# All 100 features\n","print((vect.get_feature_names_out()))"]},{"cell_type":"markdown","metadata":{"id":"J7YOMY_0HUCV"},"source":["Just like with all other models, more features does not mean a better model. So, we must tune our feature generator to remove features whose predictive capability is none or very low.\n","\n","In this case, there is roughly a 1.6% increase in accuracy when we double the n-gram size and increase our max features by 1,000-fold. Note that if we restrict it to only unigrams, then the accuracy increases even more! So, bigrams were very likely adding more noise than signal.\n","\n","In the end, by only using 16,000 unigram features we came away with a much smaller, simpler, and easier-to-think-about model which also resulted in higher accuracy."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IBKERA_hHUCW"},"outputs":[],"source":["# Include 1-grams and 2-grams, and limit the number of features.\n","\n","print('1-grams and 2-grams, up to 100K features:')\n","vect = CountVectorizer(ngram_range=(1, 2), max_features=100000)\n","tokenize_test(vect)\n","\n","print()\n","print('1-grams only, up to 100K features:')\n","vect = CountVectorizer(ngram_range=(1, 1), max_features=100000)\n","tokenize_test(vect)"]},{"cell_type":"markdown","metadata":{"id":"hLwqadyqHUCW"},"source":["- `min_df`: Float in range [0.0, 1.0] or int, default=1\n","- When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"x0sl4qm8HUCW"},"outputs":[],"source":["# Include 1-grams and 2-grams, and only include terms that appear at least two times.\n","vect = CountVectorizer(ngram_range=(1,2), min_df=2)\n","tokenize_test(vect)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pN9FkRfFHUCW"},"outputs":[],"source":["# Find the best accuracy you can testing different kwargs"]},{"cell_type":"markdown","metadata":{"id":"ezFlxxblHUCW"},"source":["<a id='textblob'></a>\n","## Introduction to TextBlob\n","\n","You should already have downloaded TextBlob, a Python library used to explore common NLP tasks. If you havenâ€™t, please return to [this step](#textblob_install) for instructions on how to do so. Weâ€™ll be using this to organize our corpora for analysis.\n","\n","As mentioned earlier, you can read more on the [TextBlob website](https://textblob.readthedocs.io/en/dev/)."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"rXdPEDf0HUCX"},"outputs":[],"source":["# Print the first review.\n","print((yelp_best_worst.text[0]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"um8ue1wnHUCX"},"outputs":[],"source":["# Save it as a TextBlob object.\n","review = TextBlob(yelp_best_worst.text[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_l8EWqzcHUCX"},"outputs":[],"source":["# List the words.\n","review.words"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CuSRM8ZdHUCX"},"outputs":[],"source":["# List the sentences.\n","review.sentences"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-UkE_LU7HUCY"},"outputs":[],"source":["# Some string methods are available.\n","review.lower()"]},{"cell_type":"markdown","metadata":{"id":"-6L-A9_AHUCY"},"source":["<a id='stem'></a>\n","## Stemming and Lemmatization\n","\n","Stemming is a crude process of removing common endings from sentences, such as \"s\", \"es\", \"ly\", \"ing\", and \"ed\".\n","\n","- **What:** Reduce a word to its base/stem/root form.\n","- **Why:** This intelligently reduces the number of features by grouping together (hopefully) related words.\n","- **Notes:**\n","    - Stemming uses a simple and fast rule-based approach.\n","    - Stemmed words are usually not shown to users (used for analysis/indexing).\n","    - Some search engines treat words with the same stem as synonyms."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jm6N0gkGHUCY"},"outputs":[],"source":["# Initialize stemmer.\n","stemmer = SnowballStemmer('english')\n","\n","# Stem each word.\n","print([stemmer.stem(word) for word in review.words])"]},{"cell_type":"markdown","metadata":{"id":"_Pb40lnwHUCZ"},"source":["Some examples you can see are \"excellent\" stemmed to \"excel\" and \"amazing\" stemmed to \"amaz\"."]},{"cell_type":"markdown","metadata":{"id":"cYAiKi_cHUCZ"},"source":["Lemmatization is a more refined process that uses specific language and grammar rules to derive the root of a word.  \n","\n","This is useful for words that do not share an obvious root such as \"better\" and \"best\".\n","\n","- **What:** Lemmatization derives the canonical form (\"lemma\") of a word.\n","- **Why:** It can be better than stemming.\n","- **Notes:** Uses a dictionary-based approach (slower than stemming)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pq0MEwhQHUCZ"},"outputs":[],"source":["import nltk\n","nltk.download('omw-1.4')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HCq2BtvqHUCa"},"outputs":[],"source":["# Assume every word is a noun.\n","print([word.lemmatize() for word in review.words])"]},{"cell_type":"markdown","metadata":{"id":"pHxCjcHDHUCa"},"source":["Some examples you can see are \"filled\" lemmatized to \"fill\" and \"was\" lemmatized to \"wa\".\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"igH1zK6nHUCa"},"outputs":[],"source":["# Assume every word is a verb.\n","print([word.lemmatize(pos='v') for word in review.words])"]},{"cell_type":"markdown","metadata":{"id":"OTVUFK--HUCb"},"source":["Some examples you can see are \"was\" lemmatized to \"be\" and \"arrived\" lemmatized to \"arrive\"."]},{"cell_type":"markdown","metadata":{"id":"wIw6nucWHUCb"},"source":["**More Lemmatization and Stemming Examples**\n","\n","|Lemmatization|Stemming|\n","|-------------|---------|\n","|shouted â†’ shout|badly â†’ bad|\n","|best â†’ good|computing â†’ comput|\n","|better â†’ good|computed â†’ comput|\n","|good â†’ good|wipes â†’ wip|\n","|wiping â†’ wipe|wiped â†’ wip|\n","|hidden â†’ hide|wiping â†’ wip|"]},{"cell_type":"markdown","metadata":{"id":"7O0jGphkHUCb"},"source":["### What do you think?\n","- What other words or phrases might cause problems with stemming? Why?\n","- What other words or phrases might cause problems with lemmatization? Why?\n","\n","----\n","\n"]},{"cell_type":"markdown","metadata":{"id":"4Ff5cZUQHUCb"},"source":["With all the available options for `CountVectorizer()`, you may wonder how to decide which to use! It's true that you can sometimes reason about which preprocessing techniques might work best. However, you will often not know for sure without trying out many different combinations and comparing their accuracies.\n","\n","> Keep in mind that you should constantly be thinking about the result of each preprocessing step instead of blindly trying them without thinking. Does each type of preprocessing \"makes sense\" with the input data you are using? Is it likely to keep intact the signal and remove noise?"]},{"cell_type":"markdown","metadata":{"id":"VVVQrTgCHUCb"},"source":["<a id='tfidf'></a>\n","## Term Frequencyâ€“Inverse Document Frequency (TFâ€“IDF)\n","\n","\n","<img src=\"https://www.dropbox.com/scl/fi/eg2n7ktyb741yc5hhlnsk/tfdif.png?rlkey=b169npkwad6nx7rtu8u6t8pas&raw=1\"  align=\"center\"/>\n","\n","While a Count Vectorizer simply totals up the number of times a \"word\" appears in a document, the more complex TF-IDF Vectorizer analyzes the uniqueness of words between documents to find distinguishing characteristics.\n","     \n","- **What:** Term frequencyâ€“inverse document frequency (TFâ€“IDF) computes the \"relative frequency\" with which a word appears in a document, compared to its frequency across all documents.\n","- **Why:** It's more useful than \"term frequency\" for identifying \"important\" words in each document (high frequency in that document, low frequency in other documents).\n","- **Notes:** It's used for search-engine scoring, text summarization, and document clustering."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zl1_wMzvHUCb"},"outputs":[],"source":["# Example documents\n","simple_train = ['call you tonight', 'Call me a cab', 'please call me... PLEASE!']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CNI_MZ1dHUCc"},"outputs":[],"source":["# Term frequency\n","vect = CountVectorizer()\n","tf = pd.DataFrame(vect.fit_transform(simple_train).toarray(), columns=vect.get_feature_names_out())\n","tf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MUZKepAWHUCc"},"outputs":[],"source":["# Document frequency\n","vect = CountVectorizer(binary=True)\n","df = vect.fit_transform(simple_train).toarray().sum(axis=0)\n","pd.DataFrame(df.reshape(1, 6), columns=vect.get_feature_names_out())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b2dRXx-YHUCc"},"outputs":[],"source":["# Term frequencyâ€“inverse document frequency (simple version)\n","tf/df"]},{"cell_type":"markdown","metadata":{"id":"SnIeDcPXHUCc"},"source":["The higher the TFâ€“IDF value, the more \"important\" the word is to that specific document. Here, \"cab\" is the most important and unique word in document 1, while \"please\" is the most important and unique word in document 2. TFâ€“IDF is often used for training as a replacement for word count."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"8bUpwa39HUCc"},"outputs":[],"source":["# TfidfVectorizer\n","vect = TfidfVectorizer()\n","pd.DataFrame(vect.fit_transform(simple_train).toarray(), columns=vect.get_feature_names_out())"]},{"cell_type":"markdown","metadata":{"id":"UFJmv1PLHUCd"},"source":["**More details:** [TFâ€“IDF is about what matters](http://planspace.org/20150524-tfidf_is_about_what_matters/)"]},{"cell_type":"markdown","metadata":{"id":"i5GUOQeYHUCd"},"source":["<a id='yelp_tfidf'></a>\n","## Using TFâ€“IDF to Summarize a Yelp Review\n","\n","Reddit's autotldr uses the [SMMRY](http://smmry.com/about) algorithm, which is based on TFâ€“IDF."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hfgxyrggHUCd"},"outputs":[],"source":["# Create a document-term matrix using TFâ€“IDF.\n","vect = TfidfVectorizer(stop_words='english')\n","\n","# Fit transform Yelp data.\n","dtm = vect.fit_transform(yelp.text)\n","features = vect.get_feature_names_out()\n","dtm.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4M-TSYKuHUCd"},"outputs":[],"source":["features"]},{"cell_type":"code","source":["'cup' in features"],"metadata":{"id":"axljIvrZFpry"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["itemindex = np.where(features == 'cup')[0][0]\n","itemindex"],"metadata":{"id":"AEeaUfhqEqjm"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"908GZNF8HUCe"},"outputs":[],"source":["def summarize():\n","\n","    # Choose a random review that is at least 300 characters.\n","    review_length = 0\n","    while review_length < 300:\n","        review_id = np.random.randint(0, len(yelp))\n","        review_text = yelp.text[review_id]\n","        review_length = len(review_text)\n","\n","    # Create a dictionary of words and their TFâ€“IDF scores.\n","    word_scores = {}\n","    for word in TextBlob(review_text).words:\n","        word = word.lower()\n","        if word in features:\n","            word_scores[word] = dtm[review_id, np.where(features == word)[0][0]]\n","    # Print words with the top five TFâ€“IDF scores.\n","    print('TOP SCORING WORDS:')\n","    top_scores = sorted(list(word_scores.items()), key=lambda x: x[1], reverse=True)[:5]\n","    for word, score in top_scores:\n","        print(word)\n","\n","    # Print five random words.\n","    print(('\\n' + 'RANDOM WORDS:'))\n","    random_words = np.random.choice(list(word_scores.keys()), size=5, replace=False)\n","    for word in random_words:\n","        print(word)\n","\n","    # Print the review.\n","    print(('\\n' + review_text))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jejcCDvmHUCe"},"outputs":[],"source":["summarize()"]},{"cell_type":"markdown","metadata":{"id":"ME0ucFsoHUCf"},"source":["<a id='sentiment'></a>\n","## Sentiment Analysis\n","\n","Understanding how positive or negative a review is. There are many ways in practice to compute a sentiment value. For example:\n","\n","- Have a list of \"positive\" words and a list of \"negative\" words and count how many occur in a document.\n","- Train a classifier given many examples of \"positive\" documents and \"negative\" documents.\n","    - Note that this technique is often just an automated way to derive the first (e.g., using bag-of-words with logistic regression, a coefficient is assigned to each word!).\n","\n","For the most accurate sentiment analysis, you will want to train a custom sentiment model based on documents that are particular to your application. Generic models (such as the one we are about to use!) often do not work as well as hoped.\n","\n","As we will do below, always make sure you double-check that the algorithm is working by manually verifying that scores correctly correspond to positive/negative reviews! Otherwise, you may be using numbers that are not accurate."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TdRKGTlLHUCf"},"outputs":[],"source":["print(review)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rV2VnNe5HUCg"},"outputs":[],"source":["# Polarity ranges from -1 (most negative) to 1 (most positive).\n","review.sentiment.polarity"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7vBgM3NeHUCg"},"outputs":[],"source":["# Define a function that accepts text and returns the polarity.\n","def detect_sentiment(text):\n","    return TextBlob(text).sentiment.polarity"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fmOjEJe9HUCg"},"outputs":[],"source":["# Create a new DataFrame column for sentiment (Warning: SLOW!).\n","import swifter\n","yelp['sentiment'] = yelp.text.swifter.apply(detect_sentiment)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RAfAObXwHUCg"},"outputs":[],"source":["yelp.head(3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m9HmhUtJHUCh"},"outputs":[],"source":["# Box plot of sentiment grouped by stars\n","yelp.boxplot(column='sentiment', by='stars')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G-RNdEJ4HUCh"},"outputs":[],"source":["# Reviews with most positive sentiment\n","yelp[yelp.sentiment == 1].text.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ma0O7SG0HUCh"},"outputs":[],"source":["# Reviews with most negative sentiment\n","yelp[yelp.sentiment == -1].text.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RJSvV39AHUCi"},"outputs":[],"source":["# Widen the column display.\n","pd.set_option('max_colwidth', 500)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lfgGTufHHUCi"},"outputs":[],"source":["# Negative sentiment in a 5-star review\n","yelp[(yelp.stars == 5) & (yelp.sentiment < -0.3)].head(1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oR8HfvBvHUCi"},"outputs":[],"source":["# Positive sentiment in a 1-star review\n","yelp[(yelp.stars == 1) & (yelp.sentiment > 0.5)].head(1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JBI44SwvHUCi"},"outputs":[],"source":["# Reset the column display width.\n","pd.reset_option('max_colwidth')"]},{"cell_type":"markdown","metadata":{"id":"YP00TMXpHUCi"},"source":["<a id='add_feat'></a>\n","## Bonus: Adding Features to a Document-Term Matrix"]},{"cell_type":"markdown","metadata":{"id":"XLrBNo2tHUCj"},"source":["Here, we will add additional features to our `CountVectorizer()`-generated feature set to hopefully improve our model.\n","\n","To make the best models, you will want to supplement the auto-generated features with new features you think might be important. After all, `CountVectorizer()` typically lowercases text and removes all associations between words. Or, you may have metadata to add in addition to just the text.\n","\n","> Remember: Although you may have hundreds of thousands of features, each data point is extremely sparse. So, if you add in a new feature, e.g., one that detects if the text is all capital letters, this new feature can still have a huge effect on the model outcome!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4MsxReXJHUCj"},"outputs":[],"source":["# Create a DataFrame that only contains the 5-star and 1-star reviews.\n","yelp_best_worst = yelp[(yelp.stars==5) | (yelp.stars==1)]\n","\n","# define X and y\n","feature_cols = ['text', 'sentiment', 'cool', 'useful', 'funny']\n","X = yelp_best_worst[feature_cols]\n","y = yelp_best_worst.stars\n","\n","# split into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"egKoOFCuHUCj"},"outputs":[],"source":["# Use CountVectorizer with text column only.\n","vect = TfidfVectorizer()\n","X_train_dtm = vect.fit_transform(X_train.text)\n","X_test_dtm = vect.transform(X_test.text)\n","print((X_train_dtm.shape))\n","print((X_test_dtm.shape))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vQYCDZJkHUCj"},"outputs":[],"source":["# Shape of other four feature columns\n","X_train.drop('text', axis=1).shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qOG24wtIHUCj"},"outputs":[],"source":["# Cast other feature columns to float and convert to a sparse matrix.\n","extra = sp.sparse.csr_matrix(X_train.drop('text', axis=1).astype(float))\n","extra.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qISNfKi-HUCk"},"outputs":[],"source":["# Combine sparse matrices.\n","X_train_dtm_extra = sp.sparse.hstack((X_train_dtm, extra))\n","X_train_dtm_extra.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"levDkDOKHUCk"},"outputs":[],"source":["# Repeat for testing set.\n","extra = sp.sparse.csr_matrix(X_test.drop('text', axis=1).astype(float))\n","X_test_dtm_extra = sp.sparse.hstack((X_test_dtm, extra))\n","X_test_dtm_extra.shape"]},{"cell_type":"code","source":["nb = MultinomialNB()\n","nb.fit(X_train_dtm_extra, y_train)\n","y_pred_class = nb.predict(X_test_dtm_extra)\n","\n","# Calculate accuracy.\n","print((metrics.accuracy_score(y_test, y_pred_class)))"],"metadata":{"id":"eFL_LEGpGTAN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Oh no! Then we scale!"],"metadata":{"id":"4eVP5a2TL634"}},{"cell_type":"code","source":["from sklearn.preprocessing import MinMaxScaler\n","min_scaler = MinMaxScaler()\n","X_train_dtm_extra_scaled = min_scaler.fit_transform(X_train_dtm_extra.toarray())\n","X_test_dtm_extra_scaled = min_scaler.fit_transform(X_test_dtm_extra.toarray())"],"metadata":{"id":"TpsNTV37LaPs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train_dtm_extra_scaled"],"metadata":{"id":"v1bopNkQvBhf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nb = MultinomialNB()\n","nb.fit(X_train_dtm_extra_scaled, y_train)\n","y_pred_class = nb.predict(X_test_dtm_extra_scaled)\n","\n","# Calculate accuracy.\n","print((metrics.accuracy_score(y_test, y_pred_class)))"],"metadata":{"id":"KleU58fDL06K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now fun stuff would happen if we mix text preprocessing from the section 1 with this models!"],"metadata":{"id":"ZYIFIJdYL9CW"}},{"cell_type":"markdown","metadata":{"id":"nFfJlo-rHUCk"},"source":["<a id='more_textblob'></a>\n","## Bonus: Fun TextBlob Features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YxzoIuTIHUCk"},"outputs":[],"source":["# Spelling correction\n","TextBlob('15 minuets late').correct()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gp37W36zHUCl"},"outputs":[],"source":["# Spellcheck\n","Word('parot').spellcheck()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GHElY1QlHUCl"},"outputs":[],"source":["# Definitions\n","Word('bank').define('v')"]},{"cell_type":"markdown","metadata":{"id":"ZcLj_k_hHUCl"},"source":["##  Naive Bayes"]},{"cell_type":"markdown","metadata":{"id":"1m10vSn-HUCl"},"source":["<a id=\"bayes\"></a>\n","\n","**What is Bayes?**  \n","Bayes, or Bayes' Theorem, is a different way to assess probability. It considers prior information in order to more accurately assess the situation.\n","\n","<img src=\"https://www.dropbox.com/scl/fi/0e2b6jrpg8kwynftk2q1x/nb.png?rlkey=8zcqaklnwtvftfv52nhttsbpv&raw=1\"  align=\"center\"/>\n"]},{"cell_type":"markdown","metadata":{"id":"MnWD0mAgHUCm"},"source":["Below is the equation for Bayes.  \n","\n","$$P(A \\ | \\ B) = \\frac {P(B \\ | \\ A) \\times P(A)} {P(B)}$$\n","\n","- **$P(A \\ | \\ B)$** : Probability of `Event A` occurring given `Event B` has occurred.\n","- **$P(B \\ | \\ A)$** : Probability of `Event B` occurring given `Event A` has occurred.\n","- **$P(A)$** : Probability of `Event A` occurring.\n","- **$P(B)$** : Probability of `Event B` occurring."]},{"cell_type":"markdown","metadata":{"id":"xf7UFlqbHUCm"},"source":["## Key Takeaways\n","\n","- The \"naive\" assumption of Naive Bayes (that the features are conditionally independent) is critical to making these calculations simple.\n","- The normalization constant (the denominator) can be ignored since it's the same for all classes.\n","- The prior probability is much less relevant once you have a lot of features.\n","\n","### Comparing Naive Bayes With Other Models\n","\n","Advantages of Naive Bayes:\n","\n","- Model training and prediction are very fast.\n","- It's somewhat interpretable.\n","- No tuning is required.\n","- Features don't need scaling.\n","- It's insensitive to irrelevant features (with enough observations).\n","- It performs better than logistic regression when the training set is very small.\n","\n","Disadvantages of Naive Bayes:\n","\n","- If \"spam\" is dependent on non-independent combinations of individual words, it may not work well.\n","- Predicted probabilities are not well calibrated.\n","- Correlated features can be problematic (due to the independence assumption).\n","- It can't handle negative features (with Multinomial Naive Bayes).\n","- It has a higher \"asymptotic error\" than logistic regression.\n","\n","-----"]},{"cell_type":"markdown","source":["## Random Forests\n","\n","<img src=\"https://www.dropbox.com/scl/fi/r206bdueik3zn6wpmebv9/rf.jpg?rlkey=i58ncoc7fg5s0ny912qqgqxyi&raw=1\"  align=\"center\"/>\n","\n"],"metadata":{"id":"47vef0cAItRA"}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestClassifier\n","rf_model = RandomForestClassifier(n_estimators=20,max_depth=4, random_state=0)\n","\n","\n","rf_model.fit(X_train_dtm, y_train)\n","\n","y_pred_class = rf_model.predict(X_test_dtm)\n","testing_accuracy = metrics.accuracy_score(y_test, y_pred_class)\n","print('the accuracy is: ',testing_accuracy)\n","testing_error = 1 - testing_accuracy\n","\n","print('the error is: ',testing_error)"],"metadata":{"id":"LJHbpBk2IwUi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lP7OAeL8HUCm"},"source":["<a id='conclusion'></a>\n","## Conclusion\n","\n","- NLP is a gigantic field.\n","- Understanding the basics broadens the types of data you can work with.\n","- Simple techniques go a long way.\n","- Use scikit-learn for NLP whenever possible.\n"]},{"cell_type":"markdown","metadata":{"id":"Os3q6Y1bHUCm"},"source":["# Now you do it\n","<img src=\"https://www.dropbox.com/scl/fi/2oqf0ur1q7ufvjhbfjjr8/spam.png?rlkey=fpvx13acltgjypf3bb6k843j4&raw=1\" align=\"center\"/>\n","\n","<img src=\"https://www.dropbox.com/scl/fi/q6sedc6g1aika01rvzec8/hands_on.jpg?rlkey=qk7bpiwwqkds648x8kmcx2ucq&raw=1\" width=\"100\" height=\"100\" align=\"right\"/>"]},{"cell_type":"markdown","metadata":{"id":"9xw5_BGTHUCm"},"source":["You are going to be analyzing mails.  These have been hand-tagged as spam or ham.\n","\n","> Build a model that predicts if the email is spam\n","\n","What is the accuracy?  Print out a heatmap to see where your model performs well, and where it performs poorly."]},{"cell_type":"code","source":["%%writefile get_data_capstone.sh\n","if [ ! -f spam.tsv ]; then\n","  wget -O spam.tsv https://www.dropbox.com/scl/fi/yy0b8tblxx787vw0ncbm4/SMSSpamCollection.tsv?rlkey=iuk84q9leb2hcyisuvqe4c1hn&dl=0\n","fi"],"metadata":{"id":"dZx3lcyb_fHr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!bash get_data_capstone.sh"],"metadata":{"id":"RKycGXXM_fHs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["raw_data = pd.read_csv(\"./spam.tsv\",sep='\\t',names=['label','text'])\n","pd.set_option('display.max_colwidth',100)\n","raw_data.head()"],"metadata":{"id":"AZRgofu56ElK"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lHqFDApKHUCn"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OEQDA5wBHUCn"},"outputs":[],"source":[]}]}