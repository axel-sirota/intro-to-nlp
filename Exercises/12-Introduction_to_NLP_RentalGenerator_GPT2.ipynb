{"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["# Transfer Learning from GPT-2\n","\n","\n","In this notebook we will generate rental descriptions by using transfer learning from GPT-2 as an encoder.\n","\n","Take it easy and check all the outputs from an inference. Remember to set is as non trainable or you can easily wait for weeks until it ends!\n","\n","You can run this lab both locally or in Colab.\n","\n","- To run in Colab just go to `https://colab.research.google.com`, sign-in and you upload this notebook. Colab has GPU access for free.\n","- To run locally just run `jupyter notebook` and access the notebook in this lab. You would need to first install the requirements in `requirements.txt`\n","\n","Follow the instructions. Good luck!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NIKJS_35hR8q"},"outputs":[],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CnbTWB1khR5s"},"outputs":[],"source":["!pip install textblob 'gensim==4.2.0' 'keras-nlp' 'transformers'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4YkshUXDhR0Y"},"outputs":[],"source":["import multiprocessing\n","import tensorflow as tf\n","import sys\n","import keras.backend as K\n","from keras.models import Sequential\n","from keras.layers import Dense, Embedding, Lambda, ELU, Conv1D, MaxPooling1D, Dropout\n","from keras import Model, Input\n","from keras.preprocessing import sequence\n","from keras.preprocessing.text import Tokenizer\n","from textblob import TextBlob, Word\n","from keras_preprocessing.sequence import pad_sequences\n","from keras.initializers import Constant\n","from tensorflow.keras.layers.experimental import preprocessing\n","from transformers import GPT2Tokenizer, TFGPT2Model\n","import keras_nlp\n","import os\n","import time\n","import sys\n","import numpy as np\n","import random\n","import os\n","import pandas as pd\n","import gensim\n","import warnings\n","import nltk\n","import pickle\n","from tensorflow.nn import leaky_relu\n","\n","import re\n","import warnings\n","from sklearn.model_selection import train_test_split\n","from textblob import TextBlob\n","from collections import defaultdict\n","\n","\n","TRACE = False\n","embedding_dim = 100\n","rnn_units = 128\n","epochs=25\n","buffer_size = 64\n","corpus_size=25000\n","test_corpus_size=5000\n","# Batch size\n","batch_size = 64\n","min_count_words = 3\n","BATCH = True\n","\n","def set_seeds_and_trace():\n","  os.environ['PYTHONHASHSEED'] = '0'\n","  np.random.seed(42)\n","  tf.random.set_seed(42)\n","  random.seed(42)\n","  if TRACE:\n","    tf.debugging.set_log_device_placement(True)\n","\n","def set_session_with_gpus_and_cores():\n","  cores = multiprocessing.cpu_count()\n","  gpus = len(tf.config.list_physical_devices('GPU'))\n","  config = tf.compat.v1.ConfigProto( device_count = {'GPU': gpus  , 'CPU': cores} , intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n","  sess = tf.compat.v1.Session(config=config)\n","  tf.compat.v1.keras.backend.set_session(sess)\n","\n","set_seeds_and_trace()\n","set_session_with_gpus_and_cores()\n","warnings.filterwarnings('ignore')\n","nltk.download('punkt')\n","tokenizer = lambda x: TextBlob(x).words"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wCyoTGiHhRN-","outputId":"702dca9a-6a74-4214-865b-c4c30e7c07ba","executionInfo":{"status":"ok","timestamp":1693968442123,"user_tz":180,"elapsed":3,"user":{"displayName":"Axel Sirota","userId":"02089179879199828401"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing get_data.sh\n"]}],"source":["%%writefile get_data.sh\n","if [ ! -f train_corpus_descriptions_airbnb.csv ]; then\n","  wget -O train_corpus_descriptions_airbnb.csv https://www.dropbox.com/scl/fi/rbrynlq7871cshi0krftj/train_corpus_descriptions_airbnb.csv?rlkey=td1pfjgqjccap0xu9g4eliube&dl=0\n","fi\n","\n","if [ ! -f test_corpus_descriptions_airbnb.csv ]; then\n","    wget -O test_corpus_descriptions_airbnb.csv https://www.dropbox.com/scl/fi/eys05bzwwnhskadqh7aux/test_corpus_descriptions_airbnb.csv?rlkey=p1zuz90khh5t7dx3hkfba1dzm&dl=0\n","fi"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EgyETfkigMHP"},"outputs":[],"source":["!bash get_data.sh"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r1amH2OpgMHQ","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["train_path = \"./train_corpus_descriptions_airbnb.csv\"\n","test_path = \"./test_corpus_descriptions_airbnb.csv\"\n","# Read, then decode for py2 compat.\n","airbnb_reviews = pd.read_csv(train_path, header=None, names=[\"review\"]).dropna().sample(n=corpus_size).reset_index(drop=True)\n","test_airbnb_reviews = pd.read_csv(test_path, header=None, names=[\"review\"]).dropna().sample(n=test_corpus_size).reset_index(drop=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I6n1DYQMmnfG"},"outputs":[],"source":["airbnb_reviews.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tyvG2sg_OPOi"},"outputs":[],"source":["def preprocess_text(text, should_join=True):\n","    text = str(text)\n","    text = ' '.join(str(word).lower() for word in tokenizer(text))\n","    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n","    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n","    if should_join:\n","      return ' '.join(gensim.utils.simple_preprocess(text))\n","    else:\n","      return gensim.utils.simple_preprocess(text)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"atRfW5TgYU-x"},"outputs":[],"source":["def get_maximum_review_length(df):\n","    maximum = 0\n","    for ix, row in df.iterrows():\n","        candidate = len(preprocess_text(row.review, should_join=False))\n","        if candidate > maximum:\n","            maximum = candidate\n","    return maximum\n","\n","\n","maximum = get_maximum_review_length(airbnb_reviews)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C6lOOvX7TZ5A"},"outputs":[],"source":["tokenizer = None # Load the GPT-2 tokenizer with max_len as maximum\n","gpt_model = None # Load the GPT-2 model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VnzXsykPVtCT"},"outputs":[],"source":["tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_ZXgNyxAe8oh"},"outputs":[],"source":["vocab_size = len(tokenizer.get_vocab())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DY5sqVpmXeiY"},"outputs":[],"source":["tokenizer(preprocess_text(airbnb_reviews.review[1], should_join=True), return_tensors='tf', padding=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eVm2DhrSt9bs"},"outputs":[],"source":["def get_ids_tensor(df):\n","  all_ids = tf.constant(np.zeros((1, maximum)), dtype='int32')\n","  for review in df.review:\n","      review = None # preprocess text noting that its output should be input to the tokenizer\n","      value = None  # Use the tokenizer with return_tensors='tf' and padding=True on the preprocessed review\n","      value = None # Pad zeroes until the tensor has size (1, maximum)\n","      value = tf.reshape(value, [1, maximum])\n","      output = tf.concat([all_ids,value], axis=0)\n","      all_ids = tf.reshape(output, [-1, maximum])\n","  return all_ids[1:]\n","\n","all_ids = get_ids_tensor(df=airbnb_reviews)\n","print(all_ids)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oTyicxUO4ER7"},"outputs":[],"source":["test_all_ids = get_ids_tensor(df=test_airbnb_reviews)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uo1Wgt5agMHV"},"outputs":[],"source":["#Prepare the dataset\n","ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n","test_ids_dataset = tf.data.Dataset.from_tensor_slices(test_all_ids)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G6s18du1gMHW"},"outputs":[],"source":["def split_input_target(sequence):\n","    input_text = sequence[:-1]\n","    target_text = sequence[1:]\n","    return input_text, target_text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_YYp6t2KgMHW","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["dataset = ids_dataset.map(split_input_target)\n","test_dataset = test_ids_dataset.map(split_input_target)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aYRoOShmgMHX","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["dataset = (\n","    dataset\n","    .shuffle(buffer_size)\n","    .batch(batch_size=batch_size, drop_remainder=True)\n",")\n","test_dataset = (\n","    test_dataset\n","    .batch(batch_size=batch_size, drop_remainder=True)\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6GR56NhN2-iL"},"outputs":[],"source":["dataset.take(1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pp6f_sr-gMHX","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["class RentalGenerator(tf.keras.Model):\n","  def __init__(self, model, vocab_size, rnn_units):\n","    super().__init__(self)\n","    self.pretrained_layer = model   # -> This is the GPT-2 model\n","    self.rnn = tf.keras.layers.LSTM(rnn_units,\n","                                   activation='tanh',\n","                                   return_sequences=True,\n","                                   return_state=True)\n","    self.dense = tf.keras.layers.Dense(vocab_size)\n","\n","  def call(self, inputs, states=None, return_state=False, training=False):\n","    # Implement the forward pass\n","    pass"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uZ0cvE_YgMHY","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["model = None # Instantiate the model and set the first layer as non trainable"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-dcNCEWogMHY","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["for input_example_batch, target_example_batch in dataset.take(1):\n","    example_batch_predictions = model(input_example_batch)\n","    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OyF-uNz-gjaa"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KOQy5P0KgMHY","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["model.summary()  # -> Validate the non-trainable parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KzX4F9z-gMHY","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["# Compile and train the model such that it early stops if the perplexity in the val set does not decrease"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9KQWnLby8G5t"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","# function for plotting loss\n","def plot_metrics(train_metric, val_metric=None, metric_name=None, title=None, ylim=5):\n","    plt.title(title)\n","    plt.ylim(0,ylim)\n","    plt.plot(train_metric,color='blue',label=metric_name)\n","    if val_metric is not None: plt.plot(val_metric,color='green',label='val_' + metric_name)\n","    plt.legend(loc=\"upper right\")\n","\n","# plot loss history\n","plot_metrics(history.history['loss'], val_metric=history.history['val_loss'], metric_name=\"Loss\", title=\"Loss\", ylim=5.0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3WQEZfLg8Hxd"},"outputs":[],"source":["plot_metrics(history.history['perplexity'], val_metric=history.history['val_perplexity'], metric_name=\"perplexity\", title=\"perplexity\", ylim=2000.0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zy0-768EgMHa","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["class OneStep(tf.keras.Model):\n","  def __init__(self, model, tokenizer, ix_to_word, temperature=1.0):\n","    super().__init__()\n","    self.temperature = temperature\n","    self.model = model\n","    self.tokenizer = tokenizer\n","    self.ix_to_word = ix_to_word\n","\n","  def expand_dims_if_neccesary(self, input):\n","    if len(input.shape) < 3:\n","      input = tf.expand_dims(input, axis=0)\n","    return input\n","\n","  def generate_one_step(self, inputs, states=None):\n","    # Convert strings to token IDs.\n","    input_words = preprocess_text(inputs, should_join=True)\n","    input_ids = self.tokenizer(input_words)\n","    # Run the model.\n","    # predicted_logits.shape is [batch, char, next_char_logits]\n","    predicted_logits, states = self.model(inputs=input_ids, states=states,\n","                                          return_state=True)\n","    # Only use the last prediction.\n","    predicted_logits = predicted_logits[:, -1, :]\n","    predicted_logits = predicted_logits/self.temperature\n","\n","    # Sample the output logits to generate token IDs.\n","    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n","    pred_id = predicted_ids.numpy()[0][0]\n","    # Convert from token ids to characters\n","    predicted_word = self.expand_dims_if_neccesary(tf.constant(self.ix_to_word[pred_id]))\n","\n","    # Return the characters and model state.\n","    return predicted_word, states"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8TkhZSSgqefM"},"outputs":[],"source":["word_to_ix = tokenizer.get_vocab()\n","ix_to_word = {ix: word for word, ix in word_to_ix.items()}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lv2UPlTWgMHa","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["one_step_model = None # Instantiate the OneStepModel\n","start = time.time()\n","states = None\n","description = tf.constant(['Midtown Sunny 2-Bedroom'])\n","\n","for n in range(200):\n","  next_word, states = None # Generate the next word and the states\n","  description = None # Append the word\n","\n","\n","result = tf.strings.join(description, separator=\" \")\n","end = time.time()\n","print(result, '\\n\\n' + '_'*80)\n","print('\\nRun time:', end - start)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tVWZTnQegMHa"},"outputs":[],"source":["tf.saved_model.save(one_step_model, 'lstm_rental_generator')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SDj6agGJCZdh"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"gpuClass":"premium"},"gpuClass":"premium","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":0}