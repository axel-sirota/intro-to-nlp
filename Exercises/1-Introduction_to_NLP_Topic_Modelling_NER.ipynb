{"cells":[{"cell_type":"markdown","metadata":{"id":"tRGSo5PuHUCC"},"source":["# Natural Language Processing\n","  \n","---\n","\n","<img src=\"https://www.dropbox.com/scl/fi/b1vbv4c4m5vikt6s08n62/nlp.png?rlkey=r5t9i1socnr84jk2slvx2pylw&raw=1\"  align=\"center\"/>"]},{"cell_type":"markdown","metadata":{"id":"-ws_4EScHUCD"},"source":["### Learning Objectives\n","- Discuss the major tasks involved with natural language processing.\n","- Discuss, on a low level, the components of natural language processing.\n","- Identify why natural language processing is difficult.\n","- Demonstrate text classification.\n","- Demonstrate common text preprocessing techniques."]},{"cell_type":"markdown","metadata":{"id":"pzfB3yQ7HUCD"},"source":["### How Do We Use NLP in Data Science?\n","\n","In data science, we are often asked to analyze unstructured text or make a predictive model using it. Unfortunately, most data science techniques require numeric data. NLP libraries provide a tool set of methods to convert unstructured text into meaningful numeric data.\n","\n","- **Analysis:** NLP techniques provide tools to allow us to understand and analyze large amounts of text. For example:\n","\n","    - Analyze the positivity/negativity of comments on different websites.\n","    - Extract key words from meeting notes and visualize how meeting topics change over time.\n","\n","- **Vectorizing for machine learning:** When building a machine learning model, we typically must transform our data into numeric features. This process of transforming non-numeric data such as natural language into numeric features is called vectorization. For example:\n","\n","    - Understanding related words. Using stemming, NLP lets us know that \"swim\", \"swims\", and \"swimming\" all refer to the same base word. This allows us to reduce the number of features used in our model.\n","    - Identifying important and unique words. Using TF-IDF (term frequency-inverse document frequency), we can identify which words are most likely to be meaningful in a document."]},{"cell_type":"markdown","metadata":{"id":"efXsM05nHUCG"},"source":["### What Is Natural Language Processing (NLP)?\n","\n","- Using computers to process (analyze, understand, generate) natural human languages.\n","- Making sense of human knowledge stored as unstructured text.\n","- Building probabilistic models using data about a language."]},{"cell_type":"markdown","metadata":{"id":"QcZ43rK4HUCH"},"source":["<img src=\"https://www.dropbox.com/scl/fi/ceuj0day17rlz5tsywhkg/siri.jpg?rlkey=k93psk3wuuru90s6kmr5l1fyg&raw=1\"  align=\"center\"/>"]},{"cell_type":"markdown","metadata":{"id":"45tO1JOIHUCH"},"source":["### What does NLU mean?\n","\n","<img src=\"https://www.dropbox.com/scl/fi/m11szeidbae8b7syyk9lb/twoway.jpg?rlkey=c2sh2q8tw2wh0owcic57u64so&raw=1\" align=\"center\"/>\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"VUFR0hnhHUCI"},"source":["### What Are Some of the Lower-Level Components?\n","\n","- **Objective:** Discuss, on a low level, the components of natural language processing.\n","\n","Unfortunately, the NLP programming libraries typically do not provide direct solutions for the high-level tasks above. Instead, they provide low-level building blocks that enable us to craft our own solutions. These include:\n","\n","- **Tokenization:** Breaking text into tokens (words, sentences, n-grams)\n","- **Stop-word removal:** a/an/the\n","- **Stemming and lemmatization:** root word\n","- **TF-IDF:** word importance\n","- **Part-of-speech tagging:** noun/verb/adjective\n","- **Named entity recognition:** person/organization/location\n","- **Spelling correction:** \"New Yrok City\"\n","- **Word sense disambiguation:** \"buy a mouse\"\n","- **Segmentation:** \"New York City subway\"\n","- **Language detection:** \"translate this page\"\n","- **Machine learning:** specialized models that work well with text"]},{"cell_type":"markdown","metadata":{"id":"7oOZHUrdHUCI"},"source":["### Why is NLP hard?\n","\n","\n","<img src=\"https://www.dropbox.com/scl/fi/bthbfwx9n1nqawxhquegp/fan.png?rlkey=483x7v41fq60agosyw71t3t4j&raw=1\"  align=\"right\"/>\n","\n","- **Objective:** Identify why natural language processing is difficult.\n","\n","Natural language processing requires an understanding of the language and the world. Several limitations of NLP are:\n","\n","- **Ambiguity**:\n","    - Hospitals Are Sued by 7 Foot Doctors\n","    - Juvenile Court to Try Shooting Defendant\n","    - Local High School Dropouts Cut in Half\n","- **Non-standard English:** text messages\n","- **Idioms:** \"throw in the towel\"\n","- **Newly coined words:** \"retweet\"\n","- **Tricky entity names:** \"Where is A Bug's Life playing?\"\n","- **World knowledge:** \"Mary and Sue are sisters\", \"Mary and Sue are mothers\""]},{"cell_type":"markdown","source":["# Introduction to Spacy and NLTK"],"metadata":{"id":"KW2KKxDIvv3S"}},{"cell_type":"markdown","metadata":{"id":"-aO1tGwWHUCF"},"source":["<a id='textblob_install'></a>\n","\n","## Install TextBlob, gensim, and swifter\n","\n","The TextBlob Python library provides a simplified interface for exploring common NLP tasks including part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation, and more.\n","\n","To proceed with the lesson, first install TextBlob, as explained below. We tend to prefer Anaconda-based installations, since they tend to be tested with our other Anaconda packages.\n","\n","**To install textblob run:**\n","\n","> `conda install -c conda-forge textblob`\n","\n","**Or:**\n","\n","> `pip install textblob`\n","\n","> `python -m textblob.download_corpora lite`\n","\n","**We will also need another set of packages: gensim, and swifter**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ipxwHoE9HUCF"},"outputs":[],"source":["!pip install --upgrade textblob spacy 'gensim==4.2.0' swifter"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VmP43_eVHUCG"},"outputs":[],"source":["!python -m textblob.download_corpora lite\n","!python -m spacy download en_core_web_sm"]},{"cell_type":"markdown","metadata":{"id":"Xm5KtvzDHUCI"},"source":["<a id='yelp_rev'></a>\n","\n","## Reading in the Yelp Reviews"]},{"cell_type":"markdown","metadata":{"id":"5yoLwmV9HUCJ"},"source":["Throughout this lesson, we will use Yelp reviews to practice and discover common low-level NLP techniques.\n","\n","You should be familiar with these terms, as they are frequently used in NLP:\n","- **corpus**: a collection of documents (derived from the Latin word for \"body\")\n","- **corpora**: plural form of corpus\n","\n","Throughout this lesson, we will use a model very popular for text classification called Naive Bayes (the \"NB\" in `BinonmialNB` and `MultinomialNB` below). If you are unfamiliar with it, know that it works exactly the same as all other models in scikit-learn! We will look extensively at the mechanics behind Naive Bayes later in the course. However, see the [appendix](#bayes) at the end of this notebook for a quick introduction."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h80K7JvGHUCJ"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import scipy as sp\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.naive_bayes import MultinomialNB         # Naive Bayes\n","from sklearn.linear_model import LogisticRegression\n","from sklearn import metrics\n","from textblob import TextBlob, Word\n","from nltk.stem.snowball import SnowballStemmer\n","\n","import spacy\n","import gensim\n","import warnings\n","import nltk\n","warnings.filterwarnings('ignore')\n","nltk.download('punkt')\n","textblob_tokenizer = lambda x: TextBlob(x).words\n"]},{"cell_type":"code","source":["%%writefile get_data.sh\n","if [ ! -f yelp.csv ]; then\n","  wget -O yelp.csv https://www.dropbox.com/s/xds4lua69b7okw8/yelp.csv?dl=0\n","fi"],"metadata":{"id":"k6RoBSKKLvJw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!bash get_data.sh"],"metadata":{"id":"OcmMkgZqLxqW"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-zG2McSWHUCK"},"outputs":[],"source":["# Read yelp.csv into a DataFrame.\n","path = './yelp.csv'\n","yelp = pd.read_csv(path)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KVOjA6xxHUCK"},"outputs":[],"source":["# The head of the original data\n","yelp.head()"]},{"cell_type":"markdown","source":["# NER and Linguistic features of Spacy"],"metadata":{"id":"829SpDNZTV9D"}},{"cell_type":"code","source":["nlp = spacy.load(\"en_core_web_sm\")\n","doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n","\n","df = pd.DataFrame([], columns=['Text',\t'Lemma',\t'POS',\t'Tag',\t'Dep',\t'Shape',\t'alpha',\t'stop'])\n","\n","\n","for ix, token in enumerate(doc):\n","    df.loc[ix] = [token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n","            token.shape_, token.is_alpha, token.is_stop]\n","df"],"metadata":{"id":"Zw5DZ4_vNb_K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","    Text: The original word text.\n","    Lemma: The base form of the word.\n","    POS: The simple UPOS part-of-speech tag.\n","    Tag: The detailed part-of-speech tag.\n","    Dep: Syntactic dependency, i.e. the relation between tokens.\n","    Shape: The word shape â€“ capitalization, punctuation, digits.\n","    is alpha: Is the token an alpha character?\n","    is stop: Is the token part of a stop list, i.e. the most common words of the language?\n","\n","\n"],"metadata":{"id":"kJl1664iOf18"}},{"cell_type":"code","source":["from spacy import displacy\n","html = displacy.render(doc, style=\"dep\")\n"],"metadata":{"id":"xnZPc8qkNk9t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import IPython\n","IPython.display.HTML(html)"],"metadata":{"id":"2-tNWp4SPGni"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import spacy\n","\n","# English pipelines include a rule-based lemmatizer\n","nlp = spacy.load(\"en_core_web_sm\")\n","lemmatizer = nlp.get_pipe(\"lemmatizer\")\n","print(lemmatizer.mode)  # 'rule'\n","\n","print([token.lemma_ for token in doc])\n","\n","# Original doc : \"Apple is looking at buying U.K. startup for $1 billion\""],"metadata":{"id":"bk53RB5NTjqZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import spacy\n","\n","df = pd.DataFrame([], columns=['Text',\t'Initial char',\t'End char',\t'Entity'])\n","\n","for ix, ent in enumerate(doc.ents):\n","    df.loc[ix] = [ent.text, ent.start_char, ent.end_char, ent.label_]\n","df"],"metadata":{"id":"nhakLANuUXxA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text = \"I saw The Beatles perform. Who did you see?\"\n","doc1 = nlp(text)\n","\n","df1 = pd.DataFrame([], columns=['Text',\t'Tag',\t'POS'])\n","\n","for i, word in enumerate(doc1):\n","  df1.loc[i] = [word, doc1[i].tag_, doc1[i].pos_]\n","df1\n","\n","\n"],"metadata":{"id":"tmz_q523ppP6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df2 = pd.DataFrame([], columns=['Text2',\t'Tag2',\t'POS2'])\n","\n","\n","# Add attribute ruler with exception for \"The Beatles\" as NNP/PROPN NNP/PROPN\n","ruler = nlp.get_pipe(\"attribute_ruler\")\n","# Pattern to match \"The Beatles\"\n","patterns = [[{\"LOWER\": \"the\"}, {\"TEXT\": \"Beatles\"}]]\n","# The attributes to assign to the matched token\n","attrs = {\"TAG\": \"NNP\", \"POS\": \"PROPN\"}\n","# Add rules to the attribute ruler\n","ruler.add(patterns=patterns, attrs=attrs, index=0)  # \"The\" in \"The Who\"\n","ruler.add(patterns=patterns, attrs=attrs, index=1)  # \"Who\" in \"The Who\"\n","\n","doc2 = nlp(text)\n","for i, word in enumerate(doc2):\n","  df2.loc[i] = [word, doc2[i].tag_, doc2[i].pos_]\n","df2"],"metadata":{"id":"0joGGswxs5gO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pd.concat([df1, df2], axis=1, )"],"metadata":{"id":"XL-Os7katNiZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Topic Modelling"],"metadata":{"id":"mV6Byqtc1JDb"}},{"cell_type":"markdown","source":["## Topic Modelling with Spacy"],"metadata":{"id":"r4JFMPCmv_xj"}},{"cell_type":"code","source":["!pip install bertopic"],"metadata":{"id":"_EWVtXObwDPW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["yelp"],"metadata":{"id":"QdE2WZklyyXw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import spacy\n","from bertopic import BERTopic\n","\n","nlp = spacy.load('en_core_web_sm', exclude=['tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer'])\n","\n","topic_model = BERTopic(embedding_model=nlp)\n","topics, probs = topic_model.fit_transform(yelp['text'])"],"metadata":{"id":"-Eqq4Bf7wHcT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","topic_model.get_topic_info()"],"metadata":{"id":"DDn1FIDE-y-S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wHUU1Y28HUCK"},"source":["## Doing Topic Modelling with LDA"]},{"cell_type":"markdown","metadata":{"id":"0h6x1m-LHUCK"},"source":["As you proceed through this section, note that text classification is done in the same way as all other classification models. First, the text is vectorized into a set of numeric features. Then, a standard machine learning classifier is applied. NLP libraries often include vectorizers and ML models that work particularly well with text.\n","\n","> We will refer to each piece of text we are trying to classify as a document.\n","> - For example, a document could refer to an email, book chapter, tweet, article, or text message.\n","\n","**Text classification is the task of predicting which category or topic a text sample is from.**\n","\n","We may want to identify:\n","- Is an article a sports or business story?\n","- Does an email have positive or negative sentiment?\n","- Is the rating of a recipe 1, 2, 3, 4, or 5 stars?\n","\n","**Predictions are often made by using the words as features and the label as the target output.**\n","\n","Starting out, we will make each unique word (across all documents) a single feature. In any given corpora, we may have hundreds of thousands of unique words, so we may have hundreds of thousands of features!\n","\n","- For a given document, the numeric value of each feature could be the number of times the word appears in the document.\n","    - So, most features will have a value of zero, resulting in a sparse matrix of features.\n","\n","- This technique for vectorizing text is referred to as a bag-of-words model.\n","    - It is called bag of words because the document's structure is lost â€” as if the words are all jumbled up in a bag.\n","    - The first step to creating a bag-of-words model is to create a vocabulary of all possible words in the corpora.\n","\n","> Alternatively, we could make each column an indicator column, which is 1 if the word is present in the document (no matter how many times) and 0 if not. This vectorization could be used to reduce the importance of repeated words. For example, a website search engine would be susceptible to spammers who load websites with repeated words. So, the search engine might use indicator columns as features rather than word counts.\n","\n","**We need to consider several things to decide if bag-of-words is appropriate.**\n","\n","- Does order of words matter?\n","- Does punctuation matter?\n","- Does upper or lower case matter?"]},{"cell_type":"markdown","metadata":{"id":"3KATt4OCHUCL"},"source":["## Demo: Text Processing in scikit-learn\n","\n","- **Objective:** Demonstrate text classification."]},{"cell_type":"markdown","metadata":{"id":"zD_003jiHUCL"},"source":["<a id='count_vec'></a>\n","\n","\n","### Creating Features Using CountVectorizer\n","\n","- **What:** Converts each document into a set of words and their counts.\n","- **Why:** To use a machine learning model, we must convert unstructured text into numeric features.\n","- **Notes:** Relatively easy with English language text, not as easy with some languages."]},{"cell_type":"markdown","metadata":{"id":"zZeJRYoGHUCL"},"source":["<img src=\"https://www.dropbox.com/scl/fi/7abked6w9kvq4az4mi7nm/cvec.png?rlkey=w1hlizux2lbkhna6f6hx4o86u&raw=1\"  align=\"center\"/>"]},{"cell_type":"code","source":["\n","# Define X and y.\n","X = yelp.text\n","y = yelp.stars\n","\n","# Split the new DataFrame into training and testing sets.\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25 ,random_state=99)"],"metadata":{"id":"Hk0F7IyaNUts"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train[:2]"],"metadata":{"id":"qDGVaY_gxwau"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8wA5T62FHUCL"},"outputs":[],"source":["# Use CountVectorizer to create document-term matrices from X_train and X_test.\n","vect = CountVectorizer()\n","X_train_dtm = vect.fit_transform(X_train)\n","X_test_dtm = vect.transform(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"6ft4ch9AHUCL"},"outputs":[],"source":["X_train.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4boUR2uDHUCM"},"outputs":[],"source":["X_train.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"__h2mrTFHUCM"},"outputs":[],"source":["X_train_dtm[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C9xeAH_0HUCM"},"outputs":[],"source":["# Rows are documents, columns are terms (aka \"tokens\" or \"features\", individual words in this situation).\n","X_train_dtm.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cl7GWFDcHUCN"},"outputs":[],"source":["# Last 50 features\n","print((vect.get_feature_names_out()[-25:]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yLwZw9ZIHUCN"},"outputs":[],"source":["# Show vectorizer vect"]},{"cell_type":"markdown","metadata":{"id":"rZqzlIvfHUCN"},"source":["[CountVectorizer documentation](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"]},{"cell_type":"markdown","metadata":{"id":"Swq4F38PHUCN"},"source":["One common method of reducing the number of features is converting all text to lowercase before generating features! Note that to a computer, `aPPle` is a different token/\"word\" than `apple`. So, by converting both to lowercase letters, it ensures fewer features will be generated. It might be useful not to convert them to lowercase if capitalization matters."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PMMQ8bnxHUCN"},"outputs":[],"source":["# Create a CountVectorizer with kwarg lowercase=False, min_df=25, token_pattern='\\w+|\\$[\\d\\.]+|\\S+' and transform the Train set\n","vect = CountVectorizer(lowercase=False, min_df=25, token_pattern='\\w+|\\$[\\d\\.]+|\\S+')\n","X_train_dtm = vect.fit_transform(X_train)\n","X_train_dtm.shape\n","# vect.get_feature_names()[-10:]"]},{"cell_type":"markdown","metadata":{"id":"7JaNr8kVHUCO"},"source":["<a id='countvectorizer-model'></a>\n","\n","\n","### Using CountVectorizer for Topic Modelling\n","![DTM](https://www.dropbox.com/scl/fi/14huaxukr29dhqxxh6gzp/DTM.png?rlkey=5ute46xkcauinbhq83lsdlzyu&raw=1)"]},{"cell_type":"code","source":["from sklearn.decomposition import LatentDirichletAllocation\n","\n","number_of_topics = 10\n","\n","model = LatentDirichletAllocation(n_components=number_of_topics, random_state=0)\n"],"metadata":{"id":"4mZVwDdpzlxf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.fit(X_train_dtm)\n"],"metadata":{"id":"Fv7LoIvFzl62"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def display_topics(model, feature_names, no_top_words):\n","    topic_dict = {}\n","    for topic_idx, topic in enumerate(model.components_):\n","        topic_dict[\"Topic %d words\" % (topic_idx)]= ['{}'.format(feature_names[i])\n","                        for i in topic.argsort()[:-no_top_words - 1:-1]]\n","        topic_dict[\"Topic %d weights\" % (topic_idx)]= ['{:.1f}'.format(topic[i])\n","                        for i in topic.argsort()[:-no_top_words - 1:-1]]\n","    return pd.DataFrame(topic_dict)\n"],"metadata":{"id":"sGexpmYvzl9_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["no_top_words = 10\n","display_topics(model, vect.get_feature_names_out(), no_top_words)"],"metadata":{"id":"0jrqVWJOzmA2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Cleaning the text data and retrying"],"metadata":{"id":"r1Y8HEWH5-eH"}},{"cell_type":"markdown","source":["Let's clean the dataset first then! Both methods failed!"],"metadata":{"id":"_cMEGYif1Abz"}},{"cell_type":"code","source":["import re\n","nltk.download('stopwords')\n","my_stopwords = nltk.corpus.stopwords.words('english')\n","word_rooter = nltk.stem.snowball.PorterStemmer(ignore_stopwords=False).stem\n","\n","my_punctuation = '!\"$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~â€¢@'\n","\n","\n","def preprocess_text(text, should_join=True):\n","    text = ' '.join(word.lower() for word in textblob_tokenizer(text))\n","    text = re.sub(r'http\\S+', '', text) # remove http links\n","    text = re.sub(r'bit.ly/\\S+', '', text) # rempve bitly links\n","    text = text.strip('[link]') # remove [links]\n","    text = re.sub('['+my_punctuation + ']+', ' ', text) # remove punctuation\n","    text = re.sub('\\s+', ' ', text) #remove double spacing\n","    text = re.sub(r\"[^a-zA-Z.,&!?]+\", r\" \", text) # only normal characters\n","    text_token_list = [word for word in text.split(' ')\n","                            if word not in my_stopwords] # remove stopwords\n","    text_token_list = [word_rooter(word) if '#' not in word else word\n","                        for word in text_token_list] # apply word rooter\n","    text = ' '.join(text_token_list)\n","    if should_join:\n","      return ' '.join(gensim.utils.simple_preprocess(text))\n","    else:\n","      return gensim.utils.simple_preprocess(text)"],"metadata":{"id":"xHoeil6UzmDf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import swifter\n","processed_reviews = yelp['text'].swifter.apply(preprocess_text)"],"metadata":{"id":"8SB2JTFF0um-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["processed_reviews = processed_reviews.rename('review')"],"metadata":{"id":"WQIVZzYG7CX6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["processed_reviews.head()"],"metadata":{"id":"EcvNsW9_0uqn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["yelp = pd.concat([yelp, processed_reviews], axis=1)"],"metadata":{"id":"4Lq8_aDA669U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["yelp"],"metadata":{"id":"Wj73bJC97k39"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X = yelp.review\n","y = yelp.stars\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25 ,random_state=99)"],"metadata":{"id":"mv4nf4b56p-G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X.shape"],"metadata":{"id":"AW9c-pBa45n7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vect = CountVectorizer(lowercase=False, min_df=25, token_pattern='\\w+|\\$[\\d\\.]+|\\S+')\n","X_train_dtm = vect.fit_transform(X_train)\n","model.fit(X_train_dtm)\n"],"metadata":{"id":"L2wEXaCl0uyI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["no_top_words = 10\n","display_topics(model, vect.get_feature_names_out(), no_top_words)"],"metadata":{"id":"51xaN1PR6EW3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Much better!! Let's see how BERT would do!"],"metadata":{"id":"owtmmR0G8J_b"}},{"cell_type":"code","source":["topics, probs = topic_model.fit_transform(yelp['review'])\n"],"metadata":{"id":"QN7PtLrG6EaQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["topic_model.get_topic_info()"],"metadata":{"id":"PYQAoeecBR_4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Os3q6Y1bHUCm"},"source":["# Now you do it\n","<img src=\"https://www.dropbox.com/scl/fi/yukrb2nsze8zku5a5sj43/bbc.jpg?rlkey=jxwx6ghoge5vd4bj6z8ze4dbc&raw=1\" width=\"400\" height=\"400\" align=\"center\"/>\n","\n","<img src=\"https://www.dropbox.com/scl/fi/q6sedc6g1aika01rvzec8/hands_on.jpg?rlkey=qk7bpiwwqkds648x8kmcx2ucq&raw=1\" width=\"100\" height=\"100\" align=\"right\"/>"]},{"cell_type":"markdown","source":["Do topic modeling on the BBC dataset! In the next section we will learn how to classify the categories as well"],"metadata":{"id":"W4QTyVJx-7Ij"}},{"cell_type":"code","source":["%%writefile get_data_capstone.sh\n","if [ ! -f bbc.csv ]; then\n","  wget -O bbc.csv https://www.dropbox.com/scl/fi/lfa2ryv86uqd3y988irfw/bbc.csv?rlkey=vtwdf6g8sejhkf75p7o36ev00&dl=0\n","fi"],"metadata":{"id":"dZx3lcyb_fHr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!bash get_data_capstone.sh"],"metadata":{"id":"RKycGXXM_fHs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pd.read_csv('./bbc.csv')"],"metadata":{"id":"AZRgofu56ElK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ZwUM_M_KBjzr"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.1"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}