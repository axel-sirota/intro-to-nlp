{"cells":[{"cell_type":"markdown","metadata":{"id":"0rqNRPALRC5T"},"source":["# Using GloVe Embedding\n","\n","In this notebook we will leverage Standford's GloVe vectors which is a pretrained embedding on 1.4B Tweets.\n","\n","Take it easy and pay attention to the main differences with before, and the non-trainable parameters. Finally, check how accurate the results now are.\n","You can run this lab both locally or in Colab.\n","\n","- To run in Colab just go to `https://colab.research.google.com`, sign-in and you upload this notebook. Colab has GPU access for free.\n","- To run locally just run `jupyter notebook` and access the notebook in this lab. You would need to first install the requirements in `requirements.txt`\n","\n","Follow the instructions. Good luck!\n"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"75x9mZNIbPAp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1694128712373,"user_tz":180,"elapsed":1238,"user":{"displayName":"Axel Sirota","userId":"02089179879199828401"}},"outputId":"362b3ac4-30f9-410b-df9f-2cdc67a9818d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Thu Sep  7 23:18:31 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   55C    P8    10W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"YUsYzWxfT8o-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1694128717263,"user_tz":180,"elapsed":3908,"user":{"displayName":"Axel Sirota","userId":"02089179879199828401"}},"outputId":"cbb76b52-b984-4fc2-d35e-153a09c83dd1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n","Collecting keras-preprocessing\n","  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.8.1)\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from keras-preprocessing) (1.23.5)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from keras-preprocessing) (1.16.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (1.3.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (2023.6.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (4.66.1)\n","Installing collected packages: keras-preprocessing\n","Successfully installed keras-preprocessing-1.1.2\n"]}],"source":["!pip install textblob keras-preprocessing"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"h3XUwgb0UBut","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1694128727838,"user_tz":180,"elapsed":10580,"user":{"displayName":"Axel Sirota","userId":"02089179879199828401"}},"outputId":"9e01423d-0be1-492b-e95d-5c832d78ea35"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]}],"source":["import multiprocessing\n","import tensorflow as tf\n","import sys\n","import keras.backend as K\n","from keras.models import Sequential\n","from keras.layers import Dense, Embedding, Lambda\n","from keras.utils import np_utils\n","from keras.preprocessing import sequence\n","from keras.preprocessing.text import Tokenizer\n","from textblob import TextBlob, Word\n","from keras_preprocessing.sequence import pad_sequences\n","from keras.initializers import Constant\n","import numpy as np\n","import random\n","import os\n","import pandas as pd\n","import gensim\n","import warnings\n","import nltk\n","\n","TRACE = False\n","embedding_dim = 100\n","epochs=2\n","batch_size = 500\n","BATCH = True\n","\n","def set_seeds_and_trace():\n","  os.environ['PYTHONHASHSEED'] = '0'\n","  np.random.seed(42)\n","  tf.random.set_seed(42)\n","  random.seed(42)\n","  if TRACE:\n","    tf.debugging.set_log_device_placement(True)\n","\n","def set_session_with_gpus_and_cores():\n","  cores = multiprocessing.cpu_count()\n","  gpus = len(tf.config.list_physical_devices('GPU'))\n","  config = tf.compat.v1.ConfigProto( device_count = {'GPU': gpus  , 'CPU': cores} , intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n","  sess = tf.compat.v1.Session(config=config)\n","  tf.compat.v1.keras.backend.set_session(sess)\n","\n","set_seeds_and_trace()\n","set_session_with_gpus_and_cores()\n","warnings.filterwarnings('ignore')\n","nltk.download('punkt')\n","tokenizer = lambda x: TextBlob(x).words"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"Tx3hZd6gUImG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1694128751438,"user_tz":180,"elapsed":460,"user":{"displayName":"Axel Sirota","userId":"02089179879199828401"}},"outputId":"95dbf4a2-98f0-4c13-ac1a-3fb90503bbfc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing get_data.sh\n"]}],"source":["%%writefile get_data.sh\n","if [ ! -f yelp.csv ]; then\n","  wget -O yelp.csv https://www.dropbox.com/s/xds4lua69b7okw8/yelp.csv?dl=0\n","fi\n","\n","if [ ! -f glove.6B.100d.txt ]; then\n","  wget -O glove.6B.100d.txt https://www.dropbox.com/s/dl1vswq2sz5f1ws/glove.6B.100d.txt?dl=0\n","fi"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"PfyMUL8nXRYP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1694128785340,"user_tz":180,"elapsed":32839,"user":{"displayName":"Axel Sirota","userId":"02089179879199828401"}},"outputId":"697d6f55-4a1d-4dd7-8690-022ed8a9d975"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-09-07 23:19:11--  https://www.dropbox.com/s/xds4lua69b7okw8/yelp.csv?dl=0\n","Resolving www.dropbox.com (www.dropbox.com)... 162.125.64.18, 2620:100:6025:18::a27d:4512\n","Connecting to www.dropbox.com (www.dropbox.com)|162.125.64.18|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: /s/raw/xds4lua69b7okw8/yelp.csv [following]\n","--2023-09-07 23:19:12--  https://www.dropbox.com/s/raw/xds4lua69b7okw8/yelp.csv\n","Reusing existing connection to www.dropbox.com:443.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://uc61889e09e30b8b06c9c9cb916e.dl.dropboxusercontent.com/cd/0/inline/CDTrULvbSK57p-NEbRJZVCM_qjOU3t0YzzxWMPNXBnOWvly1NoSLfKpjNueq4zNdWivOaRGiCcQhqa4BrCFUoGdsbJ9tU9yeRgejhlUMSAhZi4avKAw510QtNoDQkOYFWX1_keiJi813Kin7bLzLfMLq/file# [following]\n","--2023-09-07 23:19:13--  https://uc61889e09e30b8b06c9c9cb916e.dl.dropboxusercontent.com/cd/0/inline/CDTrULvbSK57p-NEbRJZVCM_qjOU3t0YzzxWMPNXBnOWvly1NoSLfKpjNueq4zNdWivOaRGiCcQhqa4BrCFUoGdsbJ9tU9yeRgejhlUMSAhZi4avKAw510QtNoDQkOYFWX1_keiJi813Kin7bLzLfMLq/file\n","Resolving uc61889e09e30b8b06c9c9cb916e.dl.dropboxusercontent.com (uc61889e09e30b8b06c9c9cb916e.dl.dropboxusercontent.com)... 162.125.69.15, 2620:100:6031:15::a27d:510f\n","Connecting to uc61889e09e30b8b06c9c9cb916e.dl.dropboxusercontent.com (uc61889e09e30b8b06c9c9cb916e.dl.dropboxusercontent.com)|162.125.69.15|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 8091185 (7.7M) [text/plain]\n","Saving to: ‘yelp.csv’\n","\n","yelp.csv            100%[===================>]   7.72M  3.81MB/s    in 2.0s    \n","\n","2023-09-07 23:19:16 (3.81 MB/s) - ‘yelp.csv’ saved [8091185/8091185]\n","\n","--2023-09-07 23:19:16--  https://www.dropbox.com/s/dl1vswq2sz5f1ws/glove.6B.100d.txt?dl=0\n","Resolving www.dropbox.com (www.dropbox.com)... 162.125.64.18, 2620:100:6031:18::a27d:5112\n","Connecting to www.dropbox.com (www.dropbox.com)|162.125.64.18|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: /s/raw/dl1vswq2sz5f1ws/glove.6B.100d.txt [following]\n","--2023-09-07 23:19:17--  https://www.dropbox.com/s/raw/dl1vswq2sz5f1ws/glove.6B.100d.txt\n","Reusing existing connection to www.dropbox.com:443.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://uc998b895ff6316e6aadeac43c1e.dl.dropboxusercontent.com/cd/0/inline/CDSwA6HuRmRo5bmr4ZtGoLuiBf2hnXU0ZbI2-CHPo78GuLb2dONeT8iap9MJl5WJMq2n2IYN-bxYN2EJ4hy7SNbqW3ih9RwTwbyEDgmWvNMix6t5u4xjLP32PjRb9Y8ME47Mun7hXZK5o7D5nrLZTAUp/file# [following]\n","--2023-09-07 23:19:18--  https://uc998b895ff6316e6aadeac43c1e.dl.dropboxusercontent.com/cd/0/inline/CDSwA6HuRmRo5bmr4ZtGoLuiBf2hnXU0ZbI2-CHPo78GuLb2dONeT8iap9MJl5WJMq2n2IYN-bxYN2EJ4hy7SNbqW3ih9RwTwbyEDgmWvNMix6t5u4xjLP32PjRb9Y8ME47Mun7hXZK5o7D5nrLZTAUp/file\n","Resolving uc998b895ff6316e6aadeac43c1e.dl.dropboxusercontent.com (uc998b895ff6316e6aadeac43c1e.dl.dropboxusercontent.com)... 162.125.69.15, 2620:100:6025:15::a27d:450f\n","Connecting to uc998b895ff6316e6aadeac43c1e.dl.dropboxusercontent.com (uc998b895ff6316e6aadeac43c1e.dl.dropboxusercontent.com)|162.125.69.15|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 347117594 (331M) [text/plain]\n","Saving to: ‘glove.6B.100d.txt’\n","\n","glove.6B.100d.txt   100%[===================>] 331.04M  14.3MB/s    in 25s     \n","\n","2023-09-07 23:19:44 (13.3 MB/s) - ‘glove.6B.100d.txt’ saved [347117594/347117594]\n","\n"]}],"source":["!bash get_data.sh"]},{"cell_type":"code","source":["! head -n 5 glove.6B.100d.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NMukzuXm8d6s","executionInfo":{"status":"ok","timestamp":1694128812687,"user_tz":180,"elapsed":1053,"user":{"displayName":"Axel Sirota","userId":"02089179879199828401"}},"outputId":"a6498ae9-b37e-42ad-9eeb-d892b48aca2f"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["the -0.038194 -0.24487 0.72812 -0.39961 0.083172 0.043953 -0.39141 0.3344 -0.57545 0.087459 0.28787 -0.06731 0.30906 -0.26384 -0.13231 -0.20757 0.33395 -0.33848 -0.31743 -0.48336 0.1464 -0.37304 0.34577 0.052041 0.44946 -0.46971 0.02628 -0.54155 -0.15518 -0.14107 -0.039722 0.28277 0.14393 0.23464 -0.31021 0.086173 0.20397 0.52624 0.17164 -0.082378 -0.71787 -0.41531 0.20335 -0.12763 0.41367 0.55187 0.57908 -0.33477 -0.36559 -0.54857 -0.062892 0.26584 0.30205 0.99775 -0.80481 -3.0243 0.01254 -0.36942 2.2167 0.72201 -0.24978 0.92136 0.034514 0.46745 1.1079 -0.19358 -0.074575 0.23353 -0.052062 -0.22044 0.057162 -0.15806 -0.30798 -0.41625 0.37972 0.15006 -0.53212 -0.2055 -1.2526 0.071624 0.70565 0.49744 -0.42063 0.26148 -1.538 -0.30223 -0.073438 -0.28312 0.37104 -0.25217 0.016215 -0.017099 -0.38984 0.87424 -0.72569 -0.51058 -0.52028 -0.1459 0.8278 0.27062\n",", -0.10767 0.11053 0.59812 -0.54361 0.67396 0.10663 0.038867 0.35481 0.06351 -0.094189 0.15786 -0.81665 0.14172 0.21939 0.58505 -0.52158 0.22783 -0.16642 -0.68228 0.3587 0.42568 0.19021 0.91963 0.57555 0.46185 0.42363 -0.095399 -0.42749 -0.16567 -0.056842 -0.29595 0.26037 -0.26606 -0.070404 -0.27662 0.15821 0.69825 0.43081 0.27952 -0.45437 -0.33801 -0.58184 0.22364 -0.5778 -0.26862 -0.20425 0.56394 -0.58524 -0.14365 -0.64218 0.0054697 -0.35248 0.16162 1.1796 -0.47674 -2.7553 -0.1321 -0.047729 1.0655 1.1034 -0.2208 0.18669 0.13177 0.15117 0.7131 -0.35215 0.91348 0.61783 0.70992 0.23955 -0.14571 -0.37859 -0.045959 -0.47368 0.2385 0.20536 -0.18996 0.32507 -1.1112 -0.36341 0.98679 -0.084776 -0.54008 0.11726 -1.0194 -0.24424 0.12771 0.013884 0.080374 -0.35414 0.34951 -0.7226 0.37549 0.4441 -0.99059 0.61214 -0.35111 -0.83155 0.45293 0.082577\n",". -0.33979 0.20941 0.46348 -0.64792 -0.38377 0.038034 0.17127 0.15978 0.46619 -0.019169 0.41479 -0.34349 0.26872 0.04464 0.42131 -0.41032 0.15459 0.022239 -0.64653 0.25256 0.043136 -0.19445 0.46516 0.45651 0.68588 0.091295 0.21875 -0.70351 0.16785 -0.35079 -0.12634 0.66384 -0.2582 0.036542 -0.13605 0.40253 0.14289 0.38132 -0.12283 -0.45886 -0.25282 -0.30432 -0.11215 -0.26182 -0.22482 -0.44554 0.2991 -0.85612 -0.14503 -0.49086 0.0082973 -0.17491 0.27524 1.4401 -0.21239 -2.8435 -0.27958 -0.45722 1.6386 0.78808 -0.55262 0.65 0.086426 0.39012 1.0632 -0.35379 0.48328 0.346 0.84174 0.098707 -0.24213 -0.27053 0.045287 -0.40147 0.11395 0.0062226 0.036673 0.018518 -1.0213 -0.20806 0.64072 -0.068763 -0.58635 0.33476 -1.1432 -0.1148 -0.25091 -0.45907 -0.096819 -0.17946 -0.063351 -0.67412 -0.068895 0.53604 -0.87773 0.31802 -0.39242 -0.23394 0.47298 -0.028803\n","of -0.1529 -0.24279 0.89837 0.16996 0.53516 0.48784 -0.58826 -0.17982 -1.3581 0.42541 0.15377 0.24215 0.13474 0.41193 0.67043 -0.56418 0.42985 -0.012183 -0.11677 0.31781 0.054177 -0.054273 0.35516 -0.30241 0.31434 -0.33846 0.71715 -0.26855 -0.15837 -0.47467 0.051581 -0.33252 0.15003 -0.1299 -0.54617 -0.37843 0.64261 0.82187 -0.080006 0.078479 -0.96976 -0.57741 0.56491 -0.39873 -0.057099 0.19743 0.065706 -0.48092 -0.20125 -0.40834 0.39456 -0.02642 -0.11838 1.012 -0.53171 -2.7474 -0.042981 -0.74849 1.7574 0.59085 0.04885 0.78267 0.38497 0.42097 0.67882 0.10337 0.6328 -0.026595 0.58647 -0.44332 0.33057 -0.12022 -0.55645 0.073611 0.20915 0.43395 -0.012761 0.089874 -1.7991 0.084808 0.77112 0.63105 -0.90685 0.60326 -1.7515 0.18596 -0.50687 -0.70203 0.66578 -0.81304 0.18712 -0.018488 -0.26757 0.727 -0.59363 -0.34839 -0.56094 -0.591 1.0039 0.20664\n","to -0.1897 0.050024 0.19084 -0.049184 -0.089737 0.21006 -0.54952 0.098377 -0.20135 0.34241 -0.092677 0.161 -0.13268 -0.2816 0.18737 -0.42959 0.96039 0.13972 -1.0781 0.40518 0.50539 -0.55064 0.4844 0.38044 -0.0029055 -0.34942 -0.099696 -0.78368 1.0363 -0.2314 -0.47121 0.57126 -0.21454 0.35958 -0.48319 1.0875 0.28524 0.12447 -0.039248 -0.076732 -0.76343 -0.32409 -0.5749 -1.0893 -0.41811 0.4512 0.12112 -0.51367 -0.13349 -1.1378 -0.28768 0.16774 0.55804 1.5387 0.018859 -2.9721 -0.24216 -0.92495 2.1992 0.28234 -0.3478 0.51621 -0.43387 0.36852 0.74573 0.072102 0.27931 0.92569 -0.050336 -0.85856 -0.1358 -0.92551 -0.33991 -1.0394 -0.067203 -0.21379 -0.4769 0.21377 -0.84008 0.052536 0.59298 0.29604 -0.67644 0.13916 -1.5504 -0.20765 0.7222 0.52056 -0.076221 -0.15194 -0.13134 0.058617 -0.31869 -0.61419 -0.62393 -0.41548 -0.038175 -0.39804 0.47647 -0.15983\n"]}]},{"cell_type":"code","execution_count":6,"metadata":{"id":"8ckCWLwTXVa0","executionInfo":{"status":"ok","timestamp":1694128785340,"user_tz":180,"elapsed":9,"user":{"displayName":"Axel Sirota","userId":"02089179879199828401"}}},"outputs":[],"source":["path = './yelp.csv'\n","yelp = pd.read_csv(path)\n","# Create a new DataFrame that only contains the 5-star and 1-star reviews to have extremes.\n","yelp_best_worst = yelp[(yelp.stars==5) | (yelp.stars==1)]\n","X = yelp_best_worst.text\n","y = yelp_best_worst.stars.map({1:0, 5:1})"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"yV8JS-8harja","executionInfo":{"status":"ok","timestamp":1694129498702,"user_tz":180,"elapsed":6708,"user":{"displayName":"Axel Sirota","userId":"02089179879199828401"}}},"outputs":[],"source":["corpus = [sentence for sentence in X.values if type(sentence) == str and len(TextBlob(sentence).words) > 3]"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"B__Of4dUXXBU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1694130501302,"user_tz":180,"elapsed":7106,"user":{"displayName":"Axel Sirota","userId":"02089179879199828401"}},"outputId":"6b07e61e-1f9b-4ea2-ba35-188da8f0de90"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 400001 word vectors.\n"]}],"source":["path_to_glove_file = \"./glove.6B.100d.txt\"\n","embeddings_index = {}\n","# Construct a function that fills the embedding_index dict for every word in the GloVe file with its coefficients.\n","# HELP: For that iterate over the Glove file (hint: check that file to view its structure first!), split the word from the numbers, and populate the dictionary with the word and the numbers as a numpy array.\n","# Hint2: check np.fromstring\n","# FILL\n","\n","with open(path_to_glove_file) as f:\n","  for line in f:\n","    word, coef = line.split(maxsplit=1)\n","    coefs = np.fromstring(coef, sep=\" \")\n","    embeddings_index[word] = coefs\n","print(\"Found %s word vectors.\" % len(embeddings_index))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fygJjt56Xhup"},"outputs":[],"source":["tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(corpus)\n","tokenized_corpus = tokenizer.texts_to_sequences(corpus)\n","nb_samples = sum(len(s) for s in corpus)\n","vocab_size = len(tokenizer.word_index) + 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pfkDahA5XnzL"},"outputs":[],"source":["print(f'First 5 corpus items are {corpus[:5]}')\n","print(f'Length of corpus is {len(corpus)}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bp6wXMtvXoJ4"},"outputs":[],"source":["num_tokens = vocab_size + 1\n","hits = 0\n","misses = 0\n","\n","embedding_matrix = np.zeros((num_tokens, embedding_dim))\n","\n","# Create a loop such that for every word in the vocabulary, if it exists in the Glove embedding, then set for that word (that means that index) the tensor of the Glove Embedding. Otherwise fill with 0\n","# FILL\n","\n","# In the end, the embedding matrix should have, for every word of our tokenizer that exists in GloVe, the tensor representation of that word"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T1nRW7STX3sr"},"outputs":[],"source":["model = Sequential()\n","model.add()  # Add the new embedding and set it as non-trainable (although you could fine-tune it if you prefer)\n","model.add()  # Add the same Lambda as before to average out the words dimension\n","model.add()  # Add a Dense layer to classify words"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CmuPaNl4YEc7"},"outputs":[],"source":["model.compile(loss='categorical_crossentropy', optimizer='adam')\n","model.summary()"]},{"cell_type":"markdown","metadata":{"id":"jQKyVsb2RC5k"},"source":["Notice the Non-trainable parameters! What we are doing is just training the softmax based on correct embeddings. This is called fine tuning the embedding.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_433LAsvYHW-"},"outputs":[],"source":["def generate_data(corpus, vocab_size, window_size=2, sentence_batch_size=10,  batch_size=250):\n","    np.random.shuffle(np.array(corpus))\n","    number_of_sentence_batches = (len(corpus) // sentence_batch_size) + 1\n","    for batch in range(number_of_sentence_batches):\n","        lower_end = batch*batch_size\n","        upper_end = (batch+1)*batch_size if batch+1 < number_of_sentence_batches else len(corpus)\n","        mini_batch_size = upper_end - lower_end\n","        maxlen = window_size*2\n","        X = []\n","        Y = []\n","        for review_id, words in enumerate(corpus[lower_end:upper_end]):\n","            L = len(words)\n","            for index, word in enumerate(words):\n","                contexts = []\n","                labels   = []\n","                s = index - window_size\n","                e = index + window_size + 1\n","\n","                contexts.append([words[i] for i in range(s, e) if 0 <= i < L and i != index])\n","                labels.append(word)\n","\n","                x = pad_sequences(contexts, maxlen=maxlen)\n","                y = np_utils.to_categorical(labels, vocab_size)\n","                X.append(x)\n","                Y.append(y)\n","        X = tf.constant(X)\n","        Y = tf.constant(Y)\n","        number_of_batches = len(X) // batch_size\n","        for real_batch in range(number_of_batches):\n","          lower_end = batch*batch_size\n","          upper_end = (batch+1)*batch_size\n","          batch_X = tf.squeeze(X[lower_end:upper_end])\n","          batch_Y = tf.squeeze(Y[lower_end:upper_end])\n","          yield (batch_X, batch_Y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t8qf2WhiYwOJ"},"outputs":[],"source":["# Re implement the method\n","def fit_model():\n","    pass"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UQz1WZF3atX3"},"outputs":[],"source":["fit_model()"]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"of4EWDZtRC5l"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"qimm9mrnRC5l"},"outputs":[],"source":["with open('./vectors.txt' ,'w') as f:\n","    f.write('{} {}\\n'.format(vocab_size-1, embedding_dim))\n","    vectors = model.get_weights()[0]\n","    for word, i in tokenizer.word_index.items():\n","        str_vec = ' '.join(map(str, list(vectors[i, :])))\n","        f.write('{} {}\\n'.format(word, str_vec))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"wL3vJsCMRC5l"},"outputs":[],"source":["w2v = gensim.models.KeyedVectors.load_word2vec_format('./vectors.txt', binary=False)"]},{"cell_type":"markdown","metadata":{"id":"C8i7kIQ4RC5m"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"ZF_n9ECtRC5m"},"outputs":[],"source":["w2v.most_similar(positive=['pizza'])"]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"gBw6pzlORC5m"},"outputs":[],"source":["w2v.most_similar(positive=['grape'])"]},{"cell_type":"markdown","metadata":{"id":"pRU29o1oRC5m"},"source":["Do you notice the difference in the accuracy? For any task first search if there are any pretrained models to use!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fMuG5oPdau7j"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4","gpuClass":"premium"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":0}